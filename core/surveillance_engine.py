import cv2
import numpy as np
import time
import threading
import json
import os
import logging
from datetime import datetime
from typing import Tuple, List, Optional, Dict, Any

# Import Plugin Manager
from core.plugin_manager import PluginManager

# Configure logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s | %(levelname)s | %(message)s")
logger = logging.getLogger("SurveillanceEngine")

# Constants (could be moved to config)
RECOGNITION_BUFFER_TIME = 7.0
BUFFER_IOU_THRESHOLD = 0.3
CONFIDENCE_MATCH = 0.45 # Default, can be overridden by config

class RecognitionBuffer:
    def __init__(self, cooldown: float, iou_threshold: float) -> None:
        self.cooldown = cooldown
        self.iou_threshold = iou_threshold
        self._entries: List[Tuple[tuple, str, float]] = [] # box, label, timestamp
        self._lock = threading.Lock()

    def clean(self, now: float) -> None:
        with self._lock:
            self._entries = [entry for entry in self._entries if now - entry[2] < self.cooldown]

    def check(self, box: tuple, now: float) -> Tuple[Optional[str], float]:
        with self._lock:
            for entry_box, label, ts in self._entries:
                iou = self.calculate_iou(box, entry_box)
                if iou >= self.iou_threshold:
                    remaining = max(0.0, self.cooldown - (now - ts))
                    if remaining > 0:
                        return label, remaining
        return None, 0.0

    def add(self, box: tuple, label: str, now: float) -> None:
        with self._lock:
            self._entries.append((box, label, now))

    @staticmethod
    def calculate_iou(box_a, box_b) -> float:
        ax1, ay1, ax2, ay2 = box_a
        bx1, by1, bx2, by2 = box_b

        inter_x1 = max(ax1, bx1)
        inter_y1 = max(ay1, by1)
        inter_x2 = min(ax2, bx2)
        inter_y2 = min(ay2, by2)

        if inter_x2 <= inter_x1 or inter_y2 <= inter_y1:
            return 0.0

        inter_area = (inter_x2 - inter_x1) * (inter_y2 - inter_y1)
        area_a = max(0, ax2 - ax1) * max(0, ay2 - ay1)
        area_b = max(0, bx2 - bx1) * max(0, by2 - by1)
        union_area = area_a + area_b - inter_area

        if union_area <= 0:
            return 0.0
        return inter_area / union_area

class SurveillanceEngine:
    def __init__(self, plugin_manager: PluginManager, config: Dict[str, Any]):
        self.pm = plugin_manager
        self.config = config
        self.stopped = False
        self.lock = threading.Lock()
        self.current_frame = None
        self.recognition_buffer = RecognitionBuffer(RECOGNITION_BUFFER_TIME, BUFFER_IOU_THRESHOLD)
        
        # Database
        self.targets_db = {}
        self.load_targets()
        
        # Threading
        self.thread = threading.Thread(target=self.run, daemon=True)
        self.thread.start()

    def load_targets(self):
        """Load targets from the JSON file generated by app.py"""
        json_path = 'data/active_surveillance_targets.json'
        self.targets_db = {}
        if os.path.exists(json_path):
            try:
                with open(json_path, 'r') as f:
                    targets = json.load(f)
                for t in targets:
                    # Store by ID or Name? Name is used for display.
                    # We need embeddings.
                    # We'll store a list of targets to iterate over.
                    self.targets_db[t['name']] = t
                logger.info(f"Loaded {len(self.targets_db)} targets.")
            except Exception as e:
                logger.error(f"Error loading targets: {e}")

    def stop(self):
        self.stopped = True
        if self.thread.is_alive():
            self.thread.join(timeout=1.0)

    def get_frame(self):
        with self.lock:
            return self.current_frame

    def save_alert(self, label, conf, frame, box):
        # Similar to original save_alert logic
        try:
            target = self.targets_db.get(label)
            if not target: return

            person_id = target['id']
            alert_dir = "data/alerts"
            images_dir = os.path.join(alert_dir, "images")
            os.makedirs(images_dir, exist_ok=True)
            
            alert_file = os.path.join(alert_dir, f"{person_id}.json")
            
            # Crop face
            x1, y1, x2, y2 = box
            h, w, _ = frame.shape
            pad = 20
            x1, y1 = max(0, x1-pad), max(0, y1-pad)
            x2, y2 = min(w, x2+pad), min(h, y2+pad)
            face_img = frame[y1:y2, x1:x2]
            
            image_filename = f"{person_id}_{int(time.time())}.jpg"
            cv2.imwrite(os.path.join(images_dir, image_filename), face_img)
            
            new_detection = {
                "timestamp": datetime.utcnow().isoformat() + "Z",
                "match_percentage": round(conf * 100, 2),
                "capture_frame": image_filename
            }
            
            alert_data = {}
            if os.path.exists(alert_file):
                with open(alert_file, 'r') as f:
                    alert_data = json.load(f)
            else:
                alert_data = target.copy()
                alert_data['detections'] = []
                if 'embeddings' in alert_data: del alert_data['embeddings']

            alert_data['detections'].append(new_detection)
            alert_data['detections'].sort(key=lambda x: x['match_percentage'], reverse=True)
            
            with open(alert_file, 'w') as f:
                json.dump(alert_data, f, indent=2)
                
            logger.info(f"Alert saved for {label}")
            
        except Exception as e:
            logger.error(f"Error saving alert: {e}")

    def compare_embedding(self, embedding, model_type='dlib'):
        """Compare embedding against DB"""
        best_label = "Unknown"
        best_conf = 0.0
        
        # Thresholds
        threshold = 0.6 # Default
        if model_type == 'dlib':
            threshold = 0.45 # Distance (lower is better)
        else:
            threshold = 0.4 # Cosine Distance (lower is better) or Similarity (higher is better)
            # ArcFace usually uses Cosine Similarity. 
            # If using InsightFace, embedding is normalized. Dot product = Cosine Sim.
            # Threshold usually ~0.3-0.4 for distance, or >0.6 for similarity.
            # Let's assume we use distance logic for consistency if possible, 
            # OR we adapt based on model type.
        
        for name, data in self.targets_db.items():
            db_emb = None
            if model_type == 'dlib' and data['embeddings'].get('dlib'):
                db_emb = np.array(data['embeddings']['dlib'])
                dist = np.linalg.norm(embedding - db_emb)
                # Convert distance to confidence (approx)
                # If dist < 0.45, it's a match.
                # Sim = 1 - (dist / threshold) ?
                if dist < threshold:
                    conf = 1.0 - (dist / 1.0) # Rough mapping
                    if conf > best_conf:
                        best_conf = conf
                        best_label = name
            
            elif model_type == 'arcface' and data['embeddings'].get('arcface'):
                db_emb = np.array(data['embeddings']['arcface'])
                # Cosine Similarity
                sim = np.dot(embedding, db_emb) / (np.linalg.norm(embedding) * np.linalg.norm(db_emb))
                if sim > 0.4: # Threshold for ArcFace
                    if sim > best_conf:
                        best_conf = sim
                        best_label = name

        return best_label, best_conf

    def run(self):
        logger.info("Surveillance Engine Started")
        while not self.stopped:
            if not self.pm.active_camera:
                time.sleep(1)
                continue

            ret, frame = self.pm.active_camera.get_frame()
            if not ret:
                time.sleep(0.1)
                continue

            current_time = time.time()
            self.recognition_buffer.clean(current_time)
            
            # Detection
            faces = self.pm.active_model.detect_faces(frame)
            
            # Determine Model Type for comparison logic
            model_type = 'dlib' if 'Dlib' in self.pm.active_model.__class__.__name__ else 'arcface'

            for face in faces:
                # Normalize Face Object
                bbox = None
                embedding = None
                
                if hasattr(face, 'bbox'): # ArcFace
                    bbox = face.bbox.astype(int)
                    embedding = face.embedding
                elif hasattr(face, 'left'): # Dlib
                    bbox = (face.left(), face.top(), face.right(), face.bottom())
                    # Dlib needs explicit generation
                    # We need to crop or pass image
                    # IFaceModel.generate_embedding takes image
                    # We can pass the whole frame? No, interface says "face_image" (crop) usually
                    # But Dlib detector returns rect on full image.
                    # Let's crop.
                    x1, y1, x2, y2 = bbox
                    h, w, _ = frame.shape
                    x1, y1 = max(0, x1), max(0, y1)
                    x2, y2 = min(w, x2), min(h, y2)
                    face_crop = frame[y1:y2, x1:x2]
                    if face_crop.size > 0:
                        # Dlib plugin expects RGB usually, let's convert in plugin or here?
                        # Plugin handles it.
                        embedding = self.pm.active_model.generate_embedding(face_crop)

                if bbox is None: continue
                
                x1, y1, x2, y2 = bbox
                
                # Check Buffer
                label, remaining = self.recognition_buffer.check(bbox, current_time)
                
                if not label and embedding is not None:
                    # Recognize
                    label, conf = self.compare_embedding(embedding, model_type)
                    
                    if label != "Unknown":
                        self.recognition_buffer.add(bbox, label, current_time)
                        # Save Alert
                        threading.Thread(target=self.save_alert, args=(label, conf, frame.copy(), bbox)).start()
                
                # Draw
                color = (0, 255, 0) if label else (0, 0, 255)
                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)
                if label:
                    text = f"{label}"
                    if remaining > 0: text += f" ({remaining:.1f}s)"
                    cv2.putText(frame, text, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)

            # Update Stream Frame
            ret, jpeg = cv2.imencode('.jpg', frame)
            if ret:
                with self.lock:
                    self.current_frame = jpeg.tobytes()

        logger.info("Surveillance Engine Stopped")
